We'll now look at some of the state-of-the-art work being done by Google's DeepMind team.

In 2014, DeepMind made waves within the machine learning community by training a deep neural network to play Atari games completely from scratch, even achieving superhuman performance. This was the first time that an AI of this scale had been trained completely autonomously, without any human input or supervision during training.

Recently, DeepMind made headlines again by creating AlphaGo, which defeated one of the world's leading Go players, Lee Sedol, in a widely publicized match in March 2016. In this section, we will explore the techniques behind the Atari AI, and how these ideas were applied to Go with such success.



In **reinforcement learning**, an **agent** takes **actions** within an **environment**. Based on these actions, the agent achieves different **states** with different **rewards**. For example, in tic-tac-toe, your reward might be 1 if you got three-in-a-row, -1 if your opponent got three-in-a-row, and 0 otherwise. Your **state space** would consist of all possible board configurations.

In the following picture, suppose that you are playing 'X'. How many actions can you take, and what is the average score of these actions?
![](https://raw.githubusercontent.com/alvinzz/brilliant-ann/master/5-5-2-1.png)
